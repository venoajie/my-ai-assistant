# ai_assistant/default_config.yml
config_version: "1.4.0"
model_selection:
  planning: "gemini-2.5-flash"
  critique: "gemini-2.5-flash"
  synthesis: "deepseek-reasoner"
  json_corrector: "gemini-2.5-flash-lite"
default_provider: "gemini"
general:
  # Path to the user's global personas directory, relative to the home directory.
  personas_directory: "personas"
  sessions_directory: ".ai_sessions"
  max_file_size_mb: 5
  universal_base_persona: "_mixins/codegen-standards-1"
  # Centralized configuration for auto-injected files and special personas
  auto_inject_files:
    - "AGENTS.md"
  critique_persona_alias: "domains/programming/pva-1"
  failure_persona_alias: "domains/programming/da-1"
  enable_llm_json_corrector: true
context_optimizer:
  max_tokens: 8000
  # If estimated input tokens exceed this, use compact prompts. Set to 0 to always use verbose prompts.
  prompt_compression_threshold: 6000
tools:
  git:
    branch_prefix: "ai"
deepseek_discount:
  start_hour: 16
  start_minute: 30
  end_hour: 0
  end_minute: 30
generation_params:
  planning:
    temperature: 0.05
    max_tokens: 512
  synthesis:
    temperature: 0.15
    topP: 0.95
    max_tokens: 8192
  critique:
    temperature: 0.1
    topP: 0.95
providers:
  gemini:
    api_key_env: "GEMINI_API_KEY"
    api_endpoint: "https://generativelanguage.googleapis.com/v1beta/openai/"
    models:
      - "gemini-2.5-pro"
      - "gemini-2.5-flash-lite" 
  deepseek:
    api_key_env: "DEEPSEEK_API_KEY"
    api_endpoint: "https://api.deepseek.com"
    models:
      - "deepseek-reasoner"
      - "deepseek-coder"
      - "deepseek-chat"
